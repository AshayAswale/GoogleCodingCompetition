{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJL0p6DkqEnX2n537Y6J+Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshayAswale/GoogleCodingCompetition/blob/master/homework5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlQsUl6nxAPH",
        "cellView": "form"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aJgwJW8zNJ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "cellView": "form",
        "outputId": "8c921ad9-2af6-41b9-dd13-7a641ec2ca3d"
      },
      "source": [
        "#@title Common code\n",
        "#################################################################\n",
        "# Insert TensorFlow code here to complete the tutorial in part 1.\n",
        "#################################################################\n",
        "\n",
        "# Load the fashion-mnist pre-shuffled train data and test data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Print training set shape - note there are 60,000 training data of image size of 28x28, 60,000 train labels)\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "# Print the number of training and test datasets\n",
        "print(x_train.shape[0], 'train set')\n",
        "print(x_test.shape[0], 'test set')\n",
        "\n",
        "# Define the text labels\n",
        "fashion_mnist_labels = [\"T-shirt/top\",  # index 0\n",
        "                        \"Trouser\",      # index 1\n",
        "                        \"Pullover\",     # index 2 \n",
        "                        \"Dress\",        # index 3 \n",
        "                        \"Coat\",         # index 4\n",
        "                        \"Sandal\",       # index 5\n",
        "                        \"Shirt\",        # index 6 \n",
        "                        \"Sneaker\",      # index 7 \n",
        "                        \"Bag\",          # index 8 \n",
        "                        \"Ankle boot\"]   # index 9\n",
        "\n",
        "# Image index, you can pick any number between 0 and 59,999\n",
        "img_index = 5\n",
        "# y_train contains the lables, ranging from 0 to 9\n",
        "label_index = y_train[img_index]\n",
        "# Print the label, for example 2 Pullover\n",
        "print (\"y = \" + str(label_index) + \" \" +(fashion_mnist_labels[label_index]))\n",
        "# # Show one of the images from the training dataset\n",
        "plt.imshow(x_train[img_index])\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Further break training data into train / validation sets (# put 5000 into validation set and keep remaining 55,000 for train)\n",
        "(x_train, x_valid) = x_train[5000:], x_train[:5000] \n",
        "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
        "\n",
        "# Reshape input data from (28, 28) to (28, 28, 1)\n",
        "w, h = 28, 28\n",
        "x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
        "x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Print training set shape\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "# Print the number of training, validation, and test datasets\n",
        "print(x_train.shape[0], 'train set')\n",
        "print(x_valid.shape[0], 'validation set')\n",
        "print(x_test.shape[0], 'test set')\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28) y_train shape: (60000,)\n",
            "Num GPUs Available:  1\n",
            "x_train shape: (60000, 28, 28) y_train shape: (60000,)\n",
            "60000 train set\n",
            "10000 test set\n",
            "y = 2 Pullover\n",
            "x_train shape: (55000, 28, 28, 1) y_train shape: (55000, 10)\n",
            "55000 train set\n",
            "5000 validation set\n",
            "10000 test set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ9zkJ7U5UVa",
        "cellView": "form"
      },
      "source": [
        "#@title Part 1\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Must define the input shape in the first layer of the neural network\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Take a look at the model summary\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
        "model.fit(x_train,\n",
        "         y_train,\n",
        "         batch_size=64,\n",
        "         epochs=10,\n",
        "         validation_data=(x_valid, y_valid),\n",
        "         callbacks=[checkpointer])\n",
        "\n",
        "# Load the weights with the best validation accuracy\n",
        "model.load_weights('model.weights.best.hdf5')\n",
        "\n",
        "# Evaluate the model on test set\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# Print test accuracy\n",
        "print('\\n', 'Test accuracy:', score[1])\n",
        "\n",
        "y_hat = model.predict(x_test)\n",
        "\n",
        "# Plot a random sample of 10 test images, their predicted labels and ground truth\n",
        "figure = plt.figure(figsize=(20, 8))\n",
        "for i, index in enumerate(np.random.choice(x_test.shape[0], size=15, replace=False)):\n",
        "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "    ax.imshow(np.squeeze(x_test[index]))\n",
        "    predict_index = np.argmax(y_hat[index])\n",
        "    true_index = np.argmax(y_test[index])\n",
        "    # Set the title for each image\n",
        "    ax.set_title(\"{} ({})\".format(fashion_mnist_labels[predict_index], \n",
        "                                  fashion_mnist_labels[true_index]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMoUfVmY551I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "e98ca5f2-8a64-47f4-9fc0-ecf6af7201d4"
      },
      "source": [
        "#@title Part 2.1 (Training Model)\n",
        "\n",
        "#################################################################\n",
        "# Insert TensorFlow code here to *train* the CNN for part 2.\n",
        "#################################################################\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# # Must define the input shape in the first layer of the neural network\n",
        "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=3, strides=1, padding='valid', input_shape=(28,28,1))) \n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, padding=\"valid\"))\n",
        "model.add(tf.keras.layers.ReLU())\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "# model.add(tf.keras.layers.Dense(1024))\n",
        "# model.add(tf.keras.layers.ReLU())\n",
        "# model.add(tf.keras.layers.Dense(10))\n",
        "# model.add(tf.keras.layers.Softmax())\n",
        "\n",
        "# Take a look at the model summary\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
        "model.fit(x_train,\n",
        "         y_train,\n",
        "         batch_size=64,\n",
        "         epochs=10,\n",
        "         validation_data=(x_valid, y_valid),\n",
        "         callbacks=[checkpointer])\n",
        "\n",
        "# Load the weights with the best validation accuracy\n",
        "model.load_weights('model.weights.best.hdf5')\n",
        "\n",
        "# Evaluate the model on test set\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# Print test accuracy\n",
        "print('\\n', 'Test accuracy:', score[1])\n",
        "\n",
        "# y_hat = model.predict(x_test)\n",
        "\n",
        "# # Plot a random sample of 10 test images, their predicted labels and ground truth\n",
        "# figure = plt.figure(figsize=(20, 8))\n",
        "# for i, index in enumerate(np.random.choice(x_test.shape[0], size=15, replace=False)):\n",
        "#     ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "#     # Display each image\n",
        "#     ax.imshow(np.squeeze(x_test[index]))\n",
        "#     predict_index = np.argmax(y_hat[index])\n",
        "#     true_index = np.argmax(y_test[index])\n",
        "#     # Set the title for each image\n",
        "#     ax.set_title(\"{} ({})\".format(fashion_mnist_labels[predict_index], \n",
        "#                                   fashion_mnist_labels[true_index]),\n",
        "#                                   color=(\"green\" if predict_index == true_index else \"red\"))\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 26, 26, 1)         10        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 1)         0         \n",
            "_________________________________________________________________\n",
            "re_lu_12 (ReLU)              (None, 13, 13, 1)         0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 169)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1024)              174080    \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 184,340\n",
            "Trainable params: 184,340\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.7678 - accuracy: 0.7499 - val_loss: 0.4528 - val_accuracy: 0.8384\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.45285, saving model to model.weights.best.hdf5\n",
            "Epoch 2/10\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.4200 - accuracy: 0.8476 - val_loss: 0.3815 - val_accuracy: 0.8640\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.45285 to 0.38150, saving model to model.weights.best.hdf5\n",
            "Epoch 3/10\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.3563 - accuracy: 0.8703 - val_loss: 0.3602 - val_accuracy: 0.8736\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.38150 to 0.36023, saving model to model.weights.best.hdf5\n",
            "Epoch 4/10\n",
            "860/860 [==============================] - 2s 3ms/step - loss: 0.3152 - accuracy: 0.8844 - val_loss: 0.3353 - val_accuracy: 0.8796\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.36023 to 0.33528, saving model to model.weights.best.hdf5\n",
            "Epoch 5/10\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2837 - accuracy: 0.8955 - val_loss: 0.3538 - val_accuracy: 0.8764\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.33528\n",
            "Epoch 6/10\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2618 - accuracy: 0.9026 - val_loss: 0.3349 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.33528 to 0.33489, saving model to model.weights.best.hdf5\n",
            "Epoch 7/10\n",
            "860/860 [==============================] - 2s 3ms/step - loss: 0.2393 - accuracy: 0.9117 - val_loss: 0.3478 - val_accuracy: 0.8812\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.33489\n",
            "Epoch 8/10\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2241 - accuracy: 0.9192 - val_loss: 0.3297 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.33489 to 0.32972, saving model to model.weights.best.hdf5\n",
            "Epoch 9/10\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2006 - accuracy: 0.9270 - val_loss: 0.3280 - val_accuracy: 0.8848\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.32972 to 0.32800, saving model to model.weights.best.hdf5\n",
            "Epoch 10/10\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1870 - accuracy: 0.9321 - val_loss: 0.3352 - val_accuracy: 0.8840\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.32800\n",
            "\n",
            " Test accuracy: 0.8835999965667725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIe2W6NeMg0t"
      },
      "source": [
        "#@title getWVector(W1_raw, x_size):\n",
        "\n",
        "def getWVector(W1_raw, b1_raw, x_size):\n",
        "  layers = W1_raw.shape[-1]\n",
        "  filter_size = W1_raw.shape[0]\n",
        "  out_size = x_size - filter_size + 1\n",
        "  # print(layers, filter_size, out_size)\n",
        "  w_rows = layers*out_size*out_size\n",
        "  w_cols = x_size*x_size\n",
        "  # print(w_rows, w_cols)\n",
        "  w1 = np.zeros((w_rows, w_cols))\n",
        "  b1 = np.zeros((layers*out_size*out_size,))\n",
        "  # print(b1)\n",
        "  \n",
        "  for layer in range(layers):\n",
        "    wts = W1_raw[:,:,0,layer]\n",
        "    # wts = wts.T\n",
        "    # print(W1_raw[:,:,0,0])\n",
        "    # b1[layer*x_size*x_size:x_size*x_size] = b1_raw[layer]\n",
        "    b1[layer*out_size*out_size:(layer+1)*out_size*out_size] = b1_raw[layer]\n",
        "    for counter_col in range(out_size):\n",
        "      for counter_row in range(out_size):\n",
        "        for w_col in range(filter_size):\n",
        "          col = counter_col*out_size + counter_row + layer * out_size*out_size\n",
        "          row = counter_col*x_size + counter_row + w_col*x_size\n",
        "          w1[col, row:row+filter_size] = wts[w_col, :]\n",
        "\n",
        "  return w1, b1\n",
        "\n",
        "# w=np.zeros((2,2,1,1))\n",
        "# w[:,:,0,0] = np.array([[11,12],[21,22]])\n",
        "# # W1_raw, b1_raw, W2_raw, b2_raw, W3_raw, b3_raw = model.trainable_variables\n",
        "# # W1, b1 = getWVector(W1_raw, b1_raw, x_size)\n",
        "# print(w[:,:,0,0])\n",
        "# b = np.array([1])\n",
        "# W1, b1 = getWVector(w, b, 3)\n",
        "# print(W1)\n",
        "# print(b1)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUcmj3SYx2xz"
      },
      "source": [
        "# print(W1[676])\r\n",
        "# print(b1.shape)\r\n",
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BUjndNIyNzG"
      },
      "source": [
        "# W1_raw = np.reshape(W1_raw, (3,3,64))\r\n",
        "print(b1_raw[0])\r\n",
        "# print(b1[0], b1[])\r\n",
        "print(b1[0], b1[675], b1[676])\r\n",
        "# print(W1_raw[:,:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRybUjngOiAA"
      },
      "source": [
        "#@title Part 2.2 (Get Weights and Biases)\n",
        "\n",
        "\n",
        "#################################################################\n",
        "# Write a method to extract the weights from the trained\n",
        "# TensorFlow model. In particular, be *careful* of the fact that\n",
        "# TensorFlow packs the convolution kernels as KxKx1xF, where\n",
        "# K is the width of the filter and F is the number of filters.\n",
        "#################################################################\n",
        "\n",
        "\n",
        "def convertWeights (model, x_size):\n",
        "    # Extract W1, b1, W2, b2, W3, b3 from model.\n",
        "    # ...\n",
        "    W1_raw, b1_raw, W2_raw, b2_raw, W3_raw, b3_raw = model.trainable_variables\n",
        "\n",
        "    W1_raw = np.array(W1_raw)\n",
        "    b1_raw = np.array(b1_raw)\n",
        "    W2 = np.array(W2_raw)\n",
        "    b2 = np.array(b2_raw)\n",
        "    W3 = np.array(W3_raw)\n",
        "    b3 = np.array(b3_raw)\n",
        "\n",
        "    W1, b1 = getWVector(W1_raw, b1_raw, x_size)\n",
        "    # W2=[]\n",
        "    # b2=[]\n",
        "    # W3=[]\n",
        "    # b3=[]\n",
        "\n",
        "\n",
        "    return W1, b1, W2, b2, W3, b3\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWyH2wuIzOVk"
      },
      "source": [
        "#@title Part 2.3 (maxPool (x, w, b, num_layers, pool_size, stride))\n",
        "\n",
        "def maxPool (x, w, b, num_layers, pool_size, stride):\n",
        "  layers = num_layers\n",
        "  out_size = (int)(math.sqrt(w.shape[0]/layers))\n",
        "\n",
        "  x_int = x.flatten()\n",
        "  interm = np.dot(w,x_int)#+b\n",
        "  # return interm \n",
        "  interm = np.reshape(interm, (layers, out_size, out_size))\n",
        "  # print(interm)\n",
        "  \n",
        "  pool_out_size = int(out_size/pool_size)\n",
        "  \n",
        "  pool = np.zeros((layers, pool_out_size, pool_out_size))\n",
        "  \n",
        "  for layer in range(layers):\n",
        "    x_l = interm[layer]\n",
        "    for i in range(pool_out_size):\n",
        "      for j in range(pool_out_size):\n",
        "        pool[layer, i, j] = np.max(x_l[i*stride:i*stride+pool_size, j*stride:j*stride+pool_size])\n",
        "        \n",
        "  return pool.flatten()\n",
        "\n",
        "# w = np.zeros((2,2,1,2))\n",
        "\n",
        "# arr = [[1,5,1,1,1],[2,2,2,2,1],[3,8,3,3,1],[4,4,0,4,1], [5,5,5,5,1]]\n",
        "# x = np.zeros((1,5,5,1))\n",
        "# x[0,:,:,0] = arr\n",
        "# print(x.shape)\n",
        "\n",
        "\n",
        "# W1, b1, W2, b2, W3, b3 = convertWeights (model,x_size)\n",
        "\n",
        "\n",
        "# w[:,:,0,0] = np.array([[11,12],[21,22]])\n",
        "# w[:,:,0,1] = np.array([[41,22],[61,12]])\n",
        "# b = np.array([1, 2])\n",
        "# W1, b1 = getWVector(w, b, 5)\n",
        "# print(W1.shape)\n",
        "# i = maxPool(x, W1, b1, 2,2,2)\n",
        "# print(i)\n",
        "\n",
        "\n",
        "# w = np.zeros((2,2,1,1))\n",
        "# x = np.array([[[1,5,1,5,1,1,1],[2,2,2,2,1,2,1],[3,8,3,3,3,3,1],[4,4,0,4,0,4,1], [5,5,5,5,1,5,1],[4,4,0,4,0,4,1],[3,8,3,3,3,3,1]]])\n",
        "# w[:,:,0,0] = np.array([[11,12],[21,22]])\n",
        "# b = np.array([1])\n",
        "# W1, b1 = getWVector(w, b, 7)\n",
        "# i = maxPool(x, W1, b1, 1,2,2)\n",
        "# print(i)\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db83nvof00kw",
        "cellView": "form"
      },
      "source": [
        "#@title getYHat(zs):\n",
        "def getYHat(zs):\n",
        "    exp_z = np.exp(zs)\n",
        "    # print(zs.shape)\n",
        "    exp_z_sums = np.sum(exp_z, axis=0)\n",
        "    y_hat = (exp_z/exp_z_sums)\n",
        "    # print(y_hat.shape)\n",
        "    return y_hat"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH9RuznvxtjJ",
        "outputId": "45f9f209-660a-4be3-fdec-d534c6f7dbeb"
      },
      "source": [
        "x_size = x_train.shape[1]\n",
        "\n",
        "count = 0\n",
        "for i in range(100):\n",
        "  yhat1 = model.predict(x_train[i:i+1,:,:,:])[0]  # Save model's output\n",
        "  # print(x_train[i:i+1,:,:,:].shape)\n",
        "\n",
        "  W1, b1, W2, b2, W3, b3 = convertWeights (model,x_size)\n",
        "  x1 = maxPool(x_train[i:i+1,:,:,:], W1, b1, 1, 2, 2)\n",
        "  h1 = np.maximum(0, x1)\n",
        "\n",
        "  x2 = np.dot(W2.T,h1)+b2\n",
        "  h2 = np.maximum(0,x2)\n",
        "\n",
        "  x3 = np.dot(W3.T, x2)+b3\n",
        "\n",
        "  # print(x3.shape)\n",
        "  y_hat = getYHat(x3)\n",
        "  int_1 = np.argmax(y_hat)\n",
        "  int_2 = np.argmax(yhat1)\n",
        "  # print(y_hat)\n",
        "  # print(yhat1)\n",
        "  if(int_1 == int_2):\n",
        "    count +=1\n",
        "print(\"accuracy:\", count)\n",
        "# print(y_hat)\n",
        "# print(yhat1)\n",
        "# print(np.argmax(y_hat))\n",
        "# print(np.argmax(yhat1))\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgly-mmAM9Vn"
      },
      "source": [
        "#@title Part 2.3\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9je6hBUb1d5S",
        "cellView": "form"
      },
      "source": [
        "#@title  Default Code\n",
        "\n",
        "yhat1 = model.predict(x_train[0:1,:,:,:])[0]  # Save model's output\n",
        "\n",
        "x_size = x_train.shape[1]\n",
        "\n",
        "W1, b1, W2, b2, W3, b3 = convertWeights (model,x_size)\n",
        "\n",
        "#################################################################\n",
        "# Below here, use numpy code ONLY (i.e., no TensorFlow) to use the\n",
        "# extracted weights to replicate the output of the TensorFlow model.\n",
        "#################################################################\n",
        "\n",
        "# Implement a fully-connected layer. For simplicity, it only needs\n",
        "# to work on one example at a time (i.e., does not need to be\n",
        "# vectorized across multiple examples).\n",
        "def fullyConnected (W, b, x):\n",
        "    pass\n",
        "\n",
        "# Implement a max-pooling layer. For simplicity, it only needs\n",
        "# to work on one example at a time (i.e., does not need to be\n",
        "# vectorized across multiple examples).\n",
        "def maxPool (x, poolingWidth):\n",
        "    pass\n",
        "\n",
        "# Implement a softmax function.\n",
        "def softmax (x):\n",
        "    pass\n",
        "\n",
        "# Implement a ReLU activation function\n",
        "def relu (x):\n",
        "    pass\n",
        "\n",
        "# Load weights from TensorFlow-trained model.\n",
        "W1, b1, W2, b2, W3, b3 = convertWeights(model)\n",
        "\n",
        "# Implement the CNN with the same architecture and weights\n",
        "# as the TensorFlow-trained model but using only numpy.\n",
        "# yhat2 = softmax(...)\n",
        "\n",
        "print(yhat1)\n",
        "print(yhat2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}